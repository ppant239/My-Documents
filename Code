import configparser
from datetime import datetime
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format
config = configparser.ConfigParser()
config.read('pp_test.cfg')
os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']
os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']
spark = SparkSession \
        .builder \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.0") \
        .getOrCreate()
input_data = "s3a://udacity-dend/song_data/A/B/C/"
#input_data = "s3a://datalakepp/song_data" # my bucket
song_data = input_data + "*.json"
#song_data = "s3a://udacity-dend/song_data/A/B/C/TRABCEI128F424C983.json"
# use this to speed up parquet write
sc = spark.sparkContext
sc._jsc.hadoopConfiguration().set("mapreduce.fileoutputcommitter.algorithm.version", "2")
df = spark.read.json(song_data)
df.printSchema()
df.show(5)
output_data = "s3a://udacity-dend/"
#df = spark.read.json(song_data)
    # extract columns to create songs table
    # Using dataframe property, create a new dataframe with required fields
songs_table = df['song_id', 'title', 'artist_id', 'year', 'duration']
    # Drop duplicates
songs_table = songs_table.dropDuplicates()
songs_table.head()

    # write songs table to parquet files partitioned by year and artist
songs_table.write.partitionBy('year', 'artist_id').parquet(os.path.join(output_data, 'songs/songs.parquet'), 'overwrite')
